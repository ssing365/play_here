import os
import time
import json
import requests
import pandas as pd  # âœ… CSV ì €ì¥ì„ ìœ„í•´ ì¶”ê°€
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import NoSuchElementException
from bs4 import BeautifulSoup
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains

# âœ… Chrome ì‹¤í–‰ ì˜µì…˜ ì¶”ê°€
options = webdriver.ChromeOptions()
options.add_argument("headless")  # ë¸Œë¼ìš°ì € ì°½ ì—†ì´ ì‹¤í–‰
options.add_argument("lang=ko_KR")

# âœ… ChromeDriver ì‹¤í–‰ (Selenium 4.x ë²„ì „ ì‚¬ìš©)
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=options)

# âœ… ì¹´ì¹´ì˜¤ API í‚¤ ì„¤ì •
KAKAO_API_KEY = "318622c7a0475756a1bb323c9a4a17cd"

def get_lat_lng_from_kakao(place_name):
    """ ì¹´ì¹´ì˜¤ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œëª…ìœ¼ë¡œ ìœ„ë„(latitude) / ê²½ë„(longitude) ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ """
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"query": place_name}

    try:
        response = requests.get(url, headers=headers, params=params)
        data = response.json()
        longitude = float(data["documents"][0]["x"])  # ê²½ë„
        latitude = float(data["documents"][0]["y"])  # ìœ„ë„
        return longitude, latitude
    except (requests.exceptions.RequestException, KeyError, IndexError):
        return None, None

def save_to_csv(results, search_keyword):
    """ í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ """
    if not results:
        return
    filename = f"{search_keyword}_data.csv".replace(" ", "_")  # ê³µë°±ì„ _ ë¡œ ë³€í™˜
    df = pd.DataFrame(results)
    df.to_csv(filename, index=False, encoding="utf-8-sig")

def main():
    global driver
    driver.implicitly_wait(4)
    driver.get("https://map.kakao.com/")

    results = []
    
    # âœ… ì—¬ëŸ¬ í‚¤ì›Œë“œë¥¼ ì‰¼í‘œ(,)ë¡œ ì…ë ¥ë°›ìŒ
    search_keywords = input("ğŸ” ê²€ìƒ‰í•  í‚¤ì›Œë“œ ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: ì„œìš¸ ë§›ì§‘, ê²½ê¸° ë§›ì§‘): ").strip()
    if not search_keywords:
        return

    max_places = int(input("ğŸ“Œ í¬ë¡¤ë§í•  ì¥ì†Œ ê°œìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip())
    if max_places <= 0:
        print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ìµœì†Œ 1ê°œ ì´ìƒì˜ ì¥ì†Œë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
        return

    # âœ… ì…ë ¥ëœ í‚¤ì›Œë“œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ê°ê° ê°œë³„ì ìœ¼ë¡œ ê²€ìƒ‰
    search_keywords_list = [kw.strip() for kw in search_keywords.split(",") if kw.strip()]
    
    for search_keyword in search_keywords_list:
        print(f"\nğŸ” '{search_keyword}' ê²€ìƒ‰ ì‹œì‘!")

        # âœ… ê²€ìƒ‰ì°½ ì´ˆê¸°í™” (ì´ì „ ê²€ìƒ‰ì–´ ì‚­ì œ í›„ ì…ë ¥)
        search_area = driver.find_element(By.XPATH, '//*[@id="search.keyword.query"]')
        search_area.clear()  # ê¸°ì¡´ ê²€ìƒ‰ì–´ ì‚­ì œ
        time.sleep(1)  # ê²€ìƒ‰ì°½ì´ ì™„ì „íˆ ì´ˆê¸°í™”ë˜ë„ë¡ ì•½ê°„ì˜ ì‹œê°„ ì¶”ê°€
        search(search_keyword, results, max_places)
    
    driver.quit()
    # âœ… ëª¨ë“  ê²°ê³¼ë¥¼ í•œ ê°œì˜ CSV íŒŒì¼ì— ì €ì¥
    save_to_csv(results, "all_places")



def search(search_keyword, results, max_places):
    global driver
    
    search_area = driver.find_element(By.XPATH, '//*[@id="search.keyword.query"]')
    search_area.send_keys(search_keyword)
    driver.find_element(By.XPATH, '//*[@id="search.keyword.submit"]').send_keys(Keys.ENTER)
    time.sleep(2)

    count = 0  # âœ… í¬ë¡¤ë§í•œ ì¥ì†Œ ê°œìˆ˜
    page_no = 1  # âœ… í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ (1í˜ì´ì§€ë¶€í„° ì‹œì‘)

    print(f"ğŸ”¹ í˜„ì¬ í˜ì´ì§€({page_no}ë²ˆ)ì—ì„œ í¬ë¡¤ë§ ì‹œì‘")
    count = crawl_current_page(results, max_places, count)
    if count >= max_places:
        return

    # âœ… "ì¥ì†Œ ë”ë³´ê¸°" ë²„íŠ¼ í´ë¦­ (ì²˜ìŒ í•œ ë²ˆë§Œ ì‹¤í–‰)
    try:
        more_button = driver.find_element(By.XPATH, '//*[@id="info.search.place.more"]')
        if more_button.is_displayed():
            driver.execute_script("arguments[0].click();", more_button)
            print("âœ… ì¥ì†Œ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì™„ë£Œ")
            time.sleep(2)
    except NoSuchElementException:
        print("âš ï¸ ì¥ì†Œ ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ")
    
    # âœ… 2~5 í˜ì´ì§€ í¬ë¡¤ë§
    for page_no in range(2, 6):  # 2ë²ˆ ~ 5ë²ˆ í˜ì´ì§€ ë°˜ë³µ
        print(f"âœ… {page_no} í˜ì´ì§€ë¡œ ì´ë™")
        page_button = driver.find_element(By.XPATH, f'//*[@id="info.search.page.no{page_no}"]')
        driver.execute_script("arguments[0].click();", page_button)
        time.sleep(2)
        count = crawl_current_page(results, max_places, count)
        if count >= max_places:
            return

    # âœ… ì´í›„ "ë‹¤ìŒ í˜ì´ì§€" ë²„íŠ¼ì„ ëˆŒëŸ¬ ë‹¤ì‹œ 1ë²ˆ í˜ì´ì§€ë¡œ ì´ë™
    while count < max_places:
        try:
            next_button = driver.find_element(By.XPATH, '//*[@id="info.search.page.next"]')
            if next_button.is_displayed():
                driver.execute_script("arguments[0].click();", next_button)
                print("âœ… ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™")
                time.sleep(2)
        except NoSuchElementException:
            print("âš ï¸ ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì—†ìŒ")
            return

        # âœ… 1ë²ˆ í˜ì´ì§€ í¬ë¡¤ë§
        print(f"ğŸ”¹ í˜„ì¬ í˜ì´ì§€(1ë²ˆ)ì—ì„œ í¬ë¡¤ë§ ì‹œì‘")
        count = crawl_current_page(results, max_places, count)
        if count >= max_places:
            return

        # âœ… 2~5 í˜ì´ì§€ í¬ë¡¤ë§ ë°˜ë³µ
        for page_no in range(2, 6):  
            print(f"âœ… {page_no} í˜ì´ì§€ë¡œ ì´ë™")
            try:
                page_button = driver.find_element(By.XPATH, f'//*[@id="info.search.page.no{page_no}"]')
                driver.execute_script("arguments[0].click();", page_button)
                time.sleep(2)
                count = crawl_current_page(results, max_places, count)
                if count >= max_places:
                    return
            except NoSuchElementException:
                print(f"âš ï¸ {page_no} í˜ì´ì§€ ë²„íŠ¼ ì—†ìŒ")
                return

def crawl_current_page(results, max_places, count):
    place_list = driver.find_elements(By.XPATH, '//*[@id="info.search.place.list"]/li')
    print(f"ğŸ”¹ í˜„ì¬ í˜ì´ì§€ì—ì„œ {len(place_list)}ê°œ ì¥ì†Œ ë°œê²¬")

    for i in range(len(place_list)):
        if count >= max_places:
            return count
        try:
            place_xpath = f'//*[@id="info.search.place.list"]/li[{i+1}]/div[5]/div[4]/a[1]'
            place_element = driver.find_element(By.XPATH, place_xpath)
            place_element.send_keys(Keys.ENTER)
            driver.switch_to.window(driver.window_handles[-1])
            time.sleep(2)

            # âœ… ê²€ìƒ‰ í‚¤ì›Œë“œ ì—†ì´ ì¥ì†Œ ë°ì´í„°ë§Œ ì¶”ê°€
            results.append(scrape_details())

            driver.close()
            driver.switch_to.window(driver.window_handles[0])
            count += 1
        except NoSuchElementException:
            break
    return count




def scrape_details():
    """ ìƒì„¸ì •ë³´ í¬ë¡¤ë§ í•¨ìˆ˜ """
    global driver
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    try:
        place_name = soup.select_one(".inner_place .tit_location").text.strip()
    except AttributeError:
        place_name = None
    try:
        location = (
            soup.select_one(".location_detail .txt_address")
            .get_text(separator=" ")  # âœ… ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜
            .replace("\n                ", " ")  # âœ… í˜¹ì‹œ ë‚¨ì•„ìˆëŠ” ì¤„ë°”ê¿ˆ ì œê±°
            .strip()  # âœ… ì–‘ìª½ ê³µë°± ì œê±°
        )
    except AttributeError:
        location = None

    try:
        descript = soup.select_one(".location_detail .txt_introduce").text.strip()
    except AttributeError:
        descript = None

    try:
        time_info = soup.select_one(".location_detail.openhour_wrap .txt_operation").text.strip()
        time_info = time_info.replace("\n", " ")  # âœ… ê°œí–‰ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    except AttributeError:
        time_info = None


    try:
        parking = "ê°€ëŠ¥" if "ico_parking" in soup.select_one(".list_facility span.ico_comm").get("class") else "ë¶ˆê°€ëŠ¥"
    except AttributeError:
        parking = "ì •ë³´ ì—†ìŒ"

    try:
        hashtag = soup.select_one(".location_detail .tag_g").text.replace("\n", ", ").strip(", ")
    except AttributeError:
        hashtag = None

    try:
        call = soup.select_one(".location_detail .txt_contact").text.strip()
    except AttributeError:
        call = None

    try:
        link = driver.current_url
    except AttributeError:
        link = None

    try:
        image = soup.select_one(".bg_present").get("style").split("url(")[-1].split(")")[0].strip("'")
        if image.startswith("//"):  # âœ… í”„ë¡œí† ì½œ ì—†ëŠ” ê²½ìš°
            image = "https:" + image  # âœ… https ì¶”ê°€
    except AttributeError:
        image = "https://via.placeholder.com/300x200?text=No+Place+Image"


    # âœ… ì§€ë„ ê´€ë ¨ ì •ë³´ (ê²½ë„, ìœ„ë„)
    longitude, latitude = get_lat_lng_from_kakao(place_name)

    try:
        placename_onmap = soup.select_one(".inner_place .tit_location").text.strip()
    except AttributeError:
        placename_onmap = None

    # âœ… ë‚ ì§œ ì •ë³´ (í˜„ì¬ ë‚ ì§œ ê¸°ì¤€)
    from datetime import datetime
    regist_date = datetime.today().strftime("%Y-%m-%d")
    edit_date = datetime.today().strftime("%Y-%m-%d")

    # âœ… ê²°ê³¼ ë°ì´í„° ì €ì¥
    place_data = {
        "pace_id": "PLACE_SEQ.NEXTVAL",
        "place_name": place_name,
        "location": location,
        "descript": descript,
        "time": time_info,
        "parking": parking,
        "call": call,
        "link": link,
        "image": image,
        "longitude": longitude,
        "latitude": latitude,
        "placename_onmap": placename_onmap,
        "regist_date": regist_date,
        "edit_date": edit_date,
        "likes" : 0,
        "hashtag": hashtag,
    }

    return place_data

if __name__ == "__main__":
    main()